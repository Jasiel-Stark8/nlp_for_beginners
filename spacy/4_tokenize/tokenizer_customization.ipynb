{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'hardest',\n",
       " 'choices',\n",
       " 'require',\n",
       " 'the',\n",
       " 'strongest',\n",
       " 'givn',\n",
       " 'will',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"The hardest choices require the strongest givn will!\")\n",
    "tokens = [token.text for token in doc if token]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'hardest',\n",
       " 'choices',\n",
       " 'require',\n",
       " 'the',\n",
       " 'strongest',\n",
       " 'giv',\n",
       " 'n',\n",
       " 'will',\n",
       " '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customize to capture slangs and custom words\n",
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"givn\", [\n",
    "    {ORTH: \"giv\"},\n",
    "    {ORTH: \"n\"}\n",
    "    ])\n",
    "doc = nlp(\"The hardest choices require the strongest givn will!\")\n",
    "tokens = [token.text for token in doc if token]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hardest choices require the strongest givn will!\n",
      "In the end it cost The great sacrifice.\n",
      "Now I can finally rest, and watch the sun rise on the great universe.\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp(\"The hardest choices require the strongest givn will! In the end it cost The great sacrifice. Now I can finally rest, and watch the sun rise on the great universe.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science.gov/\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\\\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: token has an attribute that can be used to detect a url\n",
    "def get_token(text):\n",
    "    doc = nlp(text)\n",
    "    for element in doc:\n",
    "        if element.like_url:\n",
    "            print(element)\n",
    "    return element\n",
    "\n",
    "print(get_token(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise (2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two$\n",
      "500€\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection\n",
    "doc = nlp(transactions)\n",
    "currency_transaction_list = [\n",
    "    (doc[i - 1].text + token.text if doc[i - 1].like_num else token.text + doc[i + 1].text)\n",
    "    for i, token in enumerate(doc) \n",
    "    if token.is_currency and \n",
    "       ((i > 0 and doc[i - 1].like_num) or (i < len(doc) - 1 and doc[i + 1].like_num))\n",
    "]\n",
    "for transaction in currency_transaction_list:\n",
    "    print(transaction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
